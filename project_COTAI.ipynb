{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/letriluan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/letriluan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>18460</td>\n",
       "      <td>Gerry Mullany</td>\n",
       "      <td>14/03/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>accidents</td>\n",
       "      <td>HONG KONG  ?   Hundreds of pilot whales that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>18461</td>\n",
       "      <td>Rory Smith</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>NICE, France  ?     Riv?re accepts the complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>18462</td>\n",
       "      <td>Jack Ewing</td>\n",
       "      <td>9/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>FRANKFURT  ?   Germans who never really warmed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>18463</td>\n",
       "      <td>Scott Cacciola</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>sports</td>\n",
       "      <td>Charles Oakley has strong feelings about compe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>18465</td>\n",
       "      <td>Sam Roberts</td>\n",
       "      <td>10/02/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>Hans Rosling, a Swedish doctor who transformed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               author        date  year month         topic  \\\n",
       "0    17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1    17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2    17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3    17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4    17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "..     ...                  ...         ...   ...   ...           ...   \n",
       "995  18460        Gerry Mullany  14/03/2017  2017     3     accidents   \n",
       "996  18461           Rory Smith  10/02/2017  2017     2        sports   \n",
       "997  18462           Jack Ewing   9/02/2017  2017     2      business   \n",
       "998  18463       Scott Cacciola  10/02/2017  2017     2        sports   \n",
       "999  18465          Sam Roberts  10/02/2017  2017     2     lifestyle   \n",
       "\n",
       "                                               article  \n",
       "0    PARIS  ?   When the Islamic State was about to...  \n",
       "1    Angels are everywhere in the Mu?iz family?s ap...  \n",
       "2    Finally. The Second Avenue subway opened in Ne...  \n",
       "3    WASHINGTON  ?   It?s   or   time for Republica...  \n",
       "4    For Megyn Kelly, the shift from Fox News to NB...  \n",
       "..                                                 ...  \n",
       "995  HONG KONG  ?   Hundreds of pilot whales that s...  \n",
       "996  NICE, France  ?     Riv?re accepts the complim...  \n",
       "997  FRANKFURT  ?   Germans who never really warmed...  \n",
       "998  Charles Oakley has strong feelings about compe...  \n",
       "999  Hans Rosling, a Swedish doctor who transformed...  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/letriluan/Downloads/NLP/news_dataset.csv', encoding=\"latin1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       1000 non-null   int64 \n",
      " 1   author   994 non-null    object\n",
      " 2   date     1000 non-null   object\n",
      " 3   year     1000 non-null   object\n",
      " 4   month    1000 non-null   object\n",
      " 5   topic    1000 non-null   object\n",
      " 6   article  1000 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 54.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17878.53200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>341.50184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17283.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17582.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17881.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18182.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18465.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id\n",
       "count   1000.00000\n",
       "mean   17878.53200\n",
       "std      341.50184\n",
       "min    17283.00000\n",
       "25%    17582.75000\n",
       "50%    17881.50000\n",
       "75%    18182.25000\n",
       "max    18465.00000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "author     6\n",
       "date       0\n",
       "year       0\n",
       "month      0\n",
       "topic      0\n",
       "article    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def clean_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+\\b') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    doc = nlp(\" \".join(filtered_tokens))\n",
    "    lemmatized_tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "df_new = df.copy()\n",
    "df_new[\"article\"] = df_new[\"article\"].apply(clean_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coreference Resolution utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_coreferences(text):\n",
    "    doc = nlp(text)\n",
    "    if doc._.has_coref:\n",
    "        return doc._.coref_resolved\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text matching utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_most_relevant_sentence(question, article_text):\n",
    "    text = resolve_coreferences(article_text)\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    question_embedding = model.encode(question)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(question_embedding, sentence_embeddings).squeeze()\n",
    "    most_similar_index = similarities.argmax().item()\n",
    "    confidence = similarities[most_similar_index].item()\n",
    "    \n",
    "    return sentences[most_similar_index], confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_relevant_snippets(question, relevant_sentence):\n",
    "    doc = nlp(relevant_sentence)\n",
    "    question_doc = nlp(question)\n",
    "\n",
    "    target_label = 'PERSON'  # Default to PERSON\n",
    "    \n",
    "    if any(word in question.lower() for word in ['who', 'name']):\n",
    "        target_label = 'PERSON'\n",
    "    elif any(word in question.lower() for word in ['when', 'date', 'year', 'time']):\n",
    "        target_label = 'DATE'\n",
    "    elif any(word in question.lower() for word in ['where', 'city', 'country', 'place', 'location']):\n",
    "        target_label = 'GPE'\n",
    "    elif any(word in question.lower() for word in ['what', 'company', 'organization']):\n",
    "        target_label = 'ORG'\n",
    "    elif 'how many' in question.lower():\n",
    "        target_label = 'CARDINAL' \n",
    "\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == target_label:\n",
    "            if ent.text in entities:\n",
    "                entities[ent.text] += 1\n",
    "            else:\n",
    "                entities[ent.text] = 1\n",
    "\n",
    "    if entities:\n",
    "        sorted_entities = sorted(entities.items(), key=lambda item: (-item[1], relevant_sentence.index(item[0])))\n",
    "        return sorted_entities[0][0]\n",
    "\n",
    "    return \"No relevant information found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer snippet: ('Jay Lee', 0.5828641653060913)\n"
     ]
    }
   ],
   "source": [
    "def answer_question_from_article(article_id, question, df):\n",
    "    try:\n",
    "        article_text = df.loc[df['id'] == article_id, 'article'].values[0]\n",
    "    except IndexError:\n",
    "        return \"Article not found.\"\n",
    "\n",
    "    relevant_sentence, confidence = find_most_relevant_sentence(question, article_text)\n",
    "    \n",
    "    confidence_threshold = 0.4\n",
    "    if confidence < confidence_threshold:\n",
    "        return \"High confidence answer not found.\"\n",
    "    \n",
    "    answer_snippet = extract_relevant_snippets(question, relevant_sentence)\n",
    "    return answer_snippet, confidence\n",
    "\n",
    "article_id = 17574  \n",
    "question = \"Who is the vice chairman of Samsung?\"\n",
    "answer = answer_question_from_article(article_id, question, df_new)\n",
    "print(\"Answer snippet:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article ID:  17574\n",
    "Your question:  Who run the Samsung effectively?\n",
    "Answer: High confidence answer not found.\n",
    "Article ID:  17574\n",
    "Your question:  Who is vice the chairman of Samsung?\n",
    "Answer: ('Jay Lee', 0.5779139995574951)\n",
    "Article ID:  17574\n",
    "Your question:  Who is vice the chairman of Samsung?\n",
    "Answer: ('Jay Lee', 0.5779139995574951)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  17\n",
      "Your question:  Who I am?\n",
      "Answer: Article not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who I am?\n",
      "Answer: High confidence answer not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who is the vice chairman of Samsung?\n",
      "Answer: ('Jay Lee', 0.5828641653060913)\n",
      "Article ID:  17574\n",
      "Your question:  When the vice Chaiman of Samsung will be questioned?\n",
      "Answer: ('Thursday', 0.5326929688453674)\n",
      "Article ID:  17574\n",
      "Your question:  When Jay Lee will be questioned?\n",
      "Answer: ('November', 0.5102343559265137)\n"
     ]
    }
   ],
   "source": [
    "def user_interaction():\n",
    "    while True:\n",
    "        article_id = input(\"Enter the article ID or type 'quit' to exit: \")\n",
    "        if article_id.lower() == 'quit':\n",
    "            break\n",
    "        print( 'Article ID: ', article_id)\n",
    "        question = input(\"Enter your question: \")\n",
    "\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        print('Your question: ', question)\n",
    "        try:\n",
    "            article_id = int(article_id)\n",
    "            answer = answer_question_from_article(article_id, question, df_new)\n",
    "            print(\"Answer:\", answer)\n",
    "        except ValueError:\n",
    "            print(\"Invalid article ID. Please enter a numeric ID.\")\n",
    "\n",
    "# To run the interaction\n",
    "user_interaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  18460\n",
      "Your question:  How many pilot whales that swam into a shallow New Zealand bay died overnight?\n",
      "Answer: ('Hundreds', 0.8101339936256409)\n",
      "Article ID:  18460\n",
      "Your question:  How many rescuers tried frantically to send the pilot whales back out to sea?\n",
      "Answer: ('500', 0.7037572264671326)\n",
      "Article ID:  18460\n",
      "Your question:  Where is has one of the highest rates of whale strandings?\n",
      "Answer: ('New Zealand', 0.6138548851013184)\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #Tranformers based"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer snippet: new zealand\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def answer_question_bert(question, context):\n",
    "    \"\"\"Function to answer questions using BERT directly from the context.\"\"\"\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def answer_question_bert(question, context):\n",
    "    \"\"\"Function to answer questions using BERT directly from the context, including confidence score.\"\"\"\n",
    "    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(**inputs)\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "    start_probs = softmax(answer_start_scores, dim=-1)\n",
    "    end_probs = softmax(answer_end_scores, dim=-1)\n",
    "    answer_start = torch.argmax(start_probs)\n",
    "    answer_end = torch.argmax(end_probs) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n",
    "def answer_question_from_article(article_id, question, df):\n",
    "    \"\"\"Retrieve an article by ID and use BERT to answer a question based on the article's text, including confidence.\"\"\"\n",
    "    try:\n",
    "        article_text = df.loc[df['id'] == article_id, 'article'].values[0]\n",
    "    except IndexError:\n",
    "        return \"Article not found.\"\n",
    "\n",
    "    answer = answer_question_bert(question, article_text)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  15\n",
      "Your question:  Who I am?\n",
      "Answer: Article not found.\n",
      "Article ID:  17574\n",
      "Your question:  Who I am?\n",
      "Answer: [CLS]\n",
      "Article ID:  17574\n",
      "Your question:  Who is the vice chairman of Samsung?\n",
      "Answer: jay lee\n",
      "Article ID:  17574\n",
      "Your question:  Who run the Samsung effectively?\n",
      "Answer: mr lee\n",
      "Article ID:  17574\n",
      "Your question:  When the vice Chaiman of Samsung will be questioned?\n",
      "Answer: thursday\n",
      "Article ID:  17574\n",
      "Your question:  When Jay Lee will be questioned?\n",
      "Answer: thursday\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID:  18460\n",
      "Your question:  How many pilot whales that swam into a shallow New Zealand bay died overnight?\n",
      "Answer: hundreds\n",
      "Article ID:  18460\n",
      "Your question:  How many rescuers tried frantically to send the pilot whales back out to sea?\n",
      "Answer: 500\n",
      "Article ID:  18460\n",
      "Your question:  Where is has one of the highest rates of whale strandings?\n",
      "Answer: new zealand\n"
     ]
    }
   ],
   "source": [
    "user_interaction() #BERT model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
